import asyncio
import logging
import logging.handlers
from pathlib import Path
from typing import List, Optional, Set, Tuple

from apscheduler.schedulers.asyncio import AsyncIOScheduler

from .bot.bot import TelegramBot
from .cache import AppCache
from .config import AppConfig, ConfigManager, SourceType, ForumConfig
from .database import Database, DEFAULT_FORUM
from .matcher.keyword import KeywordMatcher
from .models import Post
from .source import BaseSource, RSSSource, DiscourseSource
from .web_flask import test_cookie


def setup_logging(log_dir: Optional[Path] = None) -> None:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ

    - è¾“å‡ºåˆ° stdoutï¼ˆä¾› journald æ”¶é›†ï¼‰
    - è¾“å‡ºåˆ°æ–‡ä»¶ï¼ˆæŒ‰å¤©è½®è½¬ï¼Œä¿ç•™30å¤©ï¼‰
    """
    log_format = "%(asctime)s - %(levelname)s - %(message)s"
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    # æ¸…é™¤å·²æœ‰çš„ handlersï¼ˆé¿å…é‡å¤æ·»åŠ ï¼‰
    root_logger.handlers.clear()

    # Handler 1: stdoutï¼ˆä¾› systemd/journaldï¼‰
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(logging.Formatter(log_format))
    root_logger.addHandler(stream_handler)

    # Handler 2: æ–‡ä»¶ï¼ˆæŒ‰å¤©è½®è½¬ï¼‰
    if log_dir:
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / "app.log"
        file_handler = logging.handlers.TimedRotatingFileHandler(
            filename=log_file,
            when="midnight",      # æ¯å¤©åˆå¤œè½®è½¬
            interval=1,
            backupCount=30,       # ä¿ç•™30å¤©
            encoding="utf-8"
        )
        file_handler.setFormatter(logging.Formatter(log_format))
        file_handler.suffix = "%Y-%m-%d"  # å¤‡ä»½æ–‡ä»¶åç¼€æ ¼å¼
        root_logger.addHandler(file_handler)


# é»˜è®¤åˆå§‹åŒ–ï¼ˆä»… stdoutï¼Œæ–‡ä»¶æ—¥å¿—åœ¨ main ä¸­é…ç½®ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Suppress noisy httpx logs
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("apscheduler").setLevel(logging.WARNING)

# Batch sending configuration
BATCH_SIZE = 25  # Number of messages to send concurrently
BATCH_INTERVAL = 1.0  # Seconds between batches (Telegram rate limit ~30/sec)


def create_source(config) -> BaseSource:
    """Factory function to create data source based on config

    Args:
        config: Either AppConfig (legacy) or ForumConfig (new)
    """
    # Handle both AppConfig and ForumConfig
    if isinstance(config, ForumConfig):
        forum_config = config
    else:
        # Legacy AppConfig - get first forum or use legacy fields
        if config.forums:
            forum_config = config.forums[0]
        else:
            # Create a temporary ForumConfig from legacy fields
            forum_config = ForumConfig(
                forum_id=DEFAULT_FORUM,
                name="Linux.do",
                bot_token=config.bot_token,
                source_type=config.source_type or SourceType.RSS,
                rss_url=config.rss_url or "https://linux.do/latest.rss",
                discourse_url=config.discourse_url or "https://linux.do",
                discourse_cookie=config.discourse_cookie,
                flaresolverr_url=config.flaresolverr_url,
            )

    if forum_config.source_type == SourceType.DISCOURSE:
        if not forum_config.discourse_cookie:
            raise ValueError("Discourse source requires cookie configuration")
        return DiscourseSource(
            base_url=forum_config.discourse_url,
            cookie=forum_config.discourse_cookie,
            flaresolverr_url=forum_config.flaresolverr_url
        )
    else:
        return RSSSource(url=forum_config.rss_url)


class Application:
    """Main application that orchestrates all components for a single forum"""

    def __init__(
        self,
        forum_config: ForumConfig,
        db: Database,
        admin_chat_id: Optional[int] = None,
        config_manager: Optional[ConfigManager] = None
    ):
        self.forum_config = forum_config
        self.forum_id = forum_config.forum_id
        self.forum_name = forum_config.name
        self.admin_chat_id = admin_chat_id
        self.config_manager = config_manager
        self.db = db
        self.cache = AppCache(forum_id=self.forum_id)  # Shared cache
        self.bot = TelegramBot(
            forum_config.bot_token,
            self.db,
            forum_id=self.forum_id,
            forum_name=self.forum_name,
            cache=self.cache  # Pass shared cache to bot
        )
        self.source = create_source(forum_config)
        self.matcher = KeywordMatcher()
        self.scheduler = AsyncIOScheduler()
        self._cookie_fail_count = 0  # è¿ç»­å¤±è´¥è®¡æ•°å™¨
        self._cookie_fail_threshold = 5  # è¿ç»­å¤±è´¥é˜ˆå€¼
        self._cookie_notify_round = 0  # ç¬¬å‡ è½®é€šçŸ¥
        self._fetch_fail_count = 0  # æ‹‰å–å¤±è´¥è®¡æ•°å™¨
        self._fetch_fail_threshold = 5  # æ‹‰å–è¿ç»­å¤±è´¥é˜ˆå€¼
        self._fetch_fail_notified = False  # æ˜¯å¦å·²å‘é€æ‹‰å–å¤±è´¥å‘Šè­¦
        self.application = None  # Telegram Application instance

    def reload_config(self):
        """Hot reload configuration"""
        if not self.config_manager:
            logger.warning(f"[{self.forum_id}] æ— æ³•çƒ­æ›´æ–°ï¼šConfigManager æœªè®¾ç½®")
            return

        new_app_config = self.config_manager.load()
        if not new_app_config:
            logger.error(f"[{self.forum_id}] çƒ­æ›´æ–°å¤±è´¥ï¼šæ— æ³•åŠ è½½é…ç½®")
            return

        # Find this forum's config in the new config
        new_forum_config = new_app_config.get_forum(self.forum_id)
        if not new_forum_config:
            logger.error(f"[{self.forum_id}] çƒ­æ›´æ–°å¤±è´¥ï¼šæ‰¾ä¸åˆ°è®ºå›é…ç½®")
            return

        old_forum_config = self.forum_config

        # Update source
        self.forum_config = new_forum_config
        self.admin_chat_id = new_app_config.admin_chat_id
        self.source = create_source(new_forum_config)
        # Reset cookie invalid state on config reload
        self._cookie_fail_count = 0
        self._cookie_notify_round = 0
        # Reset fetch fail state on config reload
        self._fetch_fail_count = 0
        self._fetch_fail_notified = False
        # Invalidate cache on config change
        self.cache.clear_all()

        # Job IDs are unique per forum
        data_fetch_job_id = f"data_fetch_{self.forum_id}"
        cookie_check_job_id = f"cookie_check_{self.forum_id}"

        # æ›´æ–° scheduler å®šæ—¶ä»»åŠ¡é—´éš”
        if self.scheduler.running:
            # æ›´æ–°æ•°æ®æ‹‰å–é—´éš”
            if old_forum_config.fetch_interval != new_forum_config.fetch_interval:
                # reschedule_job ä¸æ”¯æŒ misfire_grace_timeï¼Œéœ€è¦å…ˆåˆ é™¤å†æ·»åŠ 
                self.scheduler.remove_job(data_fetch_job_id)
                self.scheduler.add_job(
                    self.fetch_and_notify,
                    "interval",
                    seconds=new_forum_config.fetch_interval,
                    id=data_fetch_job_id,
                    misfire_grace_time=None,
                    coalesce=True
                )
                logger.info(f"[{self.forum_id}] â° æ•°æ®æ‹‰å–é—´éš”å·²æ›´æ–°: {old_forum_config.fetch_interval}s â†’ {new_forum_config.fetch_interval}s")

            # æ›´æ–° Cookie æ£€æµ‹é—´éš”
            if old_forum_config.cookie_check_interval != new_forum_config.cookie_check_interval:
                if new_forum_config.cookie_check_interval > 0:
                    # å…ˆåˆ é™¤æ—§ä»»åŠ¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    job = self.scheduler.get_job(cookie_check_job_id)
                    if job:
                        self.scheduler.remove_job(cookie_check_job_id)
                    # æ·»åŠ æ–°ä»»åŠ¡
                    self.scheduler.add_job(
                        self._check_cookie_task,
                        "interval",
                        seconds=new_forum_config.cookie_check_interval,
                        id=cookie_check_job_id,
                        misfire_grace_time=None,
                        coalesce=True
                    )
                    logger.info(f"[{self.forum_id}] ğŸ” Cookie æ£€æµ‹é—´éš”å·²æ›´æ–°: {old_forum_config.cookie_check_interval}s â†’ {new_forum_config.cookie_check_interval}s")
                else:
                    # ç¦ç”¨ Cookie æ£€æµ‹
                    job = self.scheduler.get_job(cookie_check_job_id)
                    if job:
                        self.scheduler.remove_job(cookie_check_job_id)
                        logger.info(f"[{self.forum_id}] ğŸ” Cookie æ£€æµ‹å·²ç¦ç”¨")

        logger.info(f"[{self.forum_id}] ğŸ”„ é…ç½®å·²çƒ­æ›´æ–°ï¼Œæ•°æ®æº: {self.source.get_source_name()}")

    async def _notify_admin(self, message: str) -> None:
        """Send notification to admin"""
        if not self.admin_chat_id:
            logger.warning(f"[{self.forum_id}] ç®¡ç†å‘˜ chat_id æœªé…ç½®ï¼Œæ— æ³•å‘é€å‘Šè­¦")
            return

        try:
            await self.bot.send_admin_alert(self.admin_chat_id, message)
            logger.info(f"[{self.forum_id}] ğŸ“¢ å·²å‘é€ç®¡ç†å‘˜å‘Šè­¦")
        except Exception as e:
            logger.error(f"[{self.forum_id}] å‘é€ç®¡ç†å‘˜å‘Šè­¦å¤±è´¥: {e}")

    def _check_cookie_valid(self) -> dict:
        """Check if discourse cookie is valid

        Returns:
            dict with keys:
            - valid: bool
            - error_type: "cookie_invalid" | "service_error" | None
            - error: str | None
        """
        if self.forum_config.source_type != SourceType.DISCOURSE:
            return {"valid": True, "error_type": None, "error": None}

        if not self.forum_config.discourse_cookie:
            return {"valid": False, "error_type": "cookie_invalid", "error": "Cookie æœªé…ç½®"}

        # æ‰“å°å½“å‰ä½¿ç”¨çš„ cookieï¼ˆåªæ˜¾ç¤ºå‰50å­—ç¬¦ï¼‰
        cookie_preview = self.forum_config.discourse_cookie[:50] + "..." if len(self.forum_config.discourse_cookie) > 50 else self.forum_config.discourse_cookie
        logger.info(f"[{self.forum_id}] ğŸ” æ£€æµ‹ Cookie: {cookie_preview}")

        result = test_cookie(self.forum_config.discourse_cookie, self.forum_config.discourse_url, self.forum_config.flaresolverr_url)
        return result

    def _fallback_to_rss(self) -> BaseSource:
        """Create RSS fallback source (deprecated, kept for compatibility)"""
        return RSSSource(url=self.forum_config.rss_url)

    async def _check_cookie_task(self) -> None:
        """ç‹¬ç«‹çš„ Cookie æ£€æµ‹ä»»åŠ¡"""
        if self.forum_config.source_type != SourceType.DISCOURSE:
            return

        # è¿ç»­æµ‹è¯• 3 æ¬¡
        fail_count = 0
        last_result = None
        loop = asyncio.get_event_loop()
        for i in range(3):
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„ cookie æ£€æµ‹ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            result = await loop.run_in_executor(None, self._check_cookie_valid)
            last_result = result
            if not result.get("valid", False):
                fail_count += 1
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                logger.warning(f"[{self.forum_id}] âš ï¸ Cookie æ£€æµ‹å¤±è´¥ (ç¬¬ {fail_count}/3 æ¬¡): {error_msg}")
                if i < 2:  # å‰ä¸¤æ¬¡å¤±è´¥åç­‰å¾… 2 ç§’å†è¯•
                    await asyncio.sleep(2)
            else:
                break

        if fail_count == 3:
            error_type = last_result.get("error_type", "unknown") if last_result else "unknown"
            error_msg = last_result.get("error", "æœªçŸ¥é”™è¯¯") if last_result else "æœªçŸ¥é”™è¯¯"

            # æœåŠ¡é”™è¯¯ï¼ˆFlareSolverr è¶…æ—¶ç­‰ï¼‰åªè®°å½•æ—¥å¿—ï¼Œä¸å‘å‘Šè­¦
            # å› ä¸º fetch_and_notify å·²ç»æœ‰å‘Šè­¦é€»è¾‘äº†
            if error_type == "service_error":
                logger.warning(f"[{self.forum_id}] âš ï¸ Cookie æ£€æµ‹å¤±è´¥ï¼ˆæœåŠ¡é”™è¯¯ï¼‰: {error_msg}")
                return

            # Cookie çœŸæ­£å¤±æ•ˆæ‰å‘å‘Šè­¦
            self._cookie_fail_count += 1
            logger.warning(f"[{self.forum_id}] âš ï¸ Cookie è¿ç»­ 3 æ¬¡æ£€æµ‹å¤±è´¥ï¼ˆç¬¬ {self._cookie_fail_count} è½®ï¼‰: {error_msg}")
            for i in range(1, 4):
                await self._notify_admin(
                    f"âš ï¸ [{self.forum_name}] Cookie å¯èƒ½å·²å¤±æ•ˆï¼ˆç¬¬ {self._cookie_fail_count} è½®é€šçŸ¥ï¼Œç¬¬ {i}/3 éï¼‰\n\n"
                    f"Discourse Cookie è¿ç»­ 3 æ¬¡éªŒè¯å¤±è´¥ã€‚\n"
                    f"é”™è¯¯ä¿¡æ¯: {error_msg}\n\n"
                    f"å½“å‰ä»å¯æ‹‰å–å…¬å¼€æ•°æ®ï¼Œä½†éƒ¨åˆ†é™åˆ¶å†…å®¹å¯èƒ½æ— æ³•è·å–ã€‚\n\n"
                    f"{'â—' * i} è¯·æ£€æŸ¥ Cookie æ˜¯å¦éœ€è¦æ›´æ–° {'â—' * i}\n\n"
                    f"æ›´æ–°æ–¹å¼ï¼šè®¿é—®é…ç½®é¡µé¢æ›´æ–° Cookie"
                )
        else:
            # æ£€æµ‹é€šè¿‡
            logger.info(f"[{self.forum_id}] âœ… Cookie æ£€æµ‹é€šè¿‡ï¼ŒçŠ¶æ€æœ‰æ•ˆ")
            if self._cookie_fail_count > 0:
                logger.info(f"[{self.forum_id}] âœ… Cookie æ£€æµ‹æ¢å¤æ­£å¸¸ï¼ˆä¹‹å‰å¤±è´¥ {self._cookie_fail_count} è½®ï¼‰")
                await self._notify_admin(f"âœ… [{self.forum_name}] Cookie å·²æ¢å¤æœ‰æ•ˆï¼Œä¹‹å‰çš„å‘Šè­¦å¯ä»¥å¿½ç•¥äº†")
                self._cookie_fail_count = 0

    def _get_keywords_cached(self) -> List[str]:
        """Get keywords with caching (or direct DB if cache disabled)"""
        if not self.forum_config.cache_enabled:
            return self.db.get_all_keywords(forum=self.forum_id)
        cached = self.cache.get_keywords()
        if cached is not None:
            return cached
        keywords = self.db.get_all_keywords(forum=self.forum_id)
        self.cache.set_keywords(keywords)
        return keywords

    def _get_subscribe_all_users_cached(self) -> List[int]:
        """Get subscribe_all users with caching (or direct DB if cache disabled)"""
        if not self.forum_config.cache_enabled:
            return self.db.get_all_subscribe_all_users(forum=self.forum_id)
        cached = self.cache.get_subscribe_all_users()
        if cached is not None:
            return cached
        users = self.db.get_all_subscribe_all_users(forum=self.forum_id)
        self.cache.set_subscribe_all_users(users)
        return users

    def _get_subscribers_cached(self, keyword: str) -> List[int]:
        """Get subscribers for a keyword with caching (or direct DB if cache disabled)"""
        if not self.forum_config.cache_enabled:
            return self.db.get_subscribers_by_keyword(keyword, forum=self.forum_id)
        cached = self.cache.get_subscribers(keyword)
        if cached is not None:
            return cached
        subscribers = self.db.get_subscribers_by_keyword(keyword, forum=self.forum_id)
        self.cache.set_subscribers(keyword, subscribers)
        return subscribers

    def _get_subscribed_authors_cached(self) -> List[str]:
        """Get subscribed authors with caching (or direct DB if cache disabled)"""
        if not self.forum_config.cache_enabled:
            return self.db.get_all_subscribed_authors(forum=self.forum_id)
        cached = self.cache.get_authors()
        if cached is not None:
            return cached
        authors = self.db.get_all_subscribed_authors(forum=self.forum_id)
        self.cache.set_authors(authors)
        return authors

    def _get_author_subscribers_cached(self, author: str) -> List[int]:
        """Get subscribers for an author with caching (or direct DB if cache disabled)"""
        if not self.forum_config.cache_enabled:
            return self.db.get_subscribers_by_author(author, forum=self.forum_id)
        cached = self.cache.get_author_subscribers(author)
        if cached is not None:
            return cached
        subscribers = self.db.get_subscribers_by_author(author, forum=self.forum_id)
        self.cache.set_author_subscribers(author, subscribers)
        return subscribers

    async def _send_batch(self, tasks: List[Tuple]) -> int:
        """Send a batch of notifications concurrently.

        Args:
            tasks: List of (chat_id, post, keyword_or_none) tuples

        Returns:
            Number of successfully sent notifications
        """
        if not tasks:
            return 0

        async def send_one(chat_id: int, post: Post, keyword: Optional[str]) -> bool:
            try:
                if keyword:
                    success = await self.bot.send_notification(
                        chat_id, post.title, post.link, keyword
                    )
                else:
                    success = await self.bot.send_notification_all(
                        chat_id, post.title, post.link
                    )
                if success:
                    # Record notification in DB
                    self.db.add_notification(chat_id, post.id, keyword or "__ALL__", forum=self.forum_id)
                return success
            except Exception as e:
                logger.error(f"[{self.forum_id}] å‘é€å¤±è´¥ {chat_id}: {e}")
                return False

        # Execute batch concurrently
        results = await asyncio.gather(
            *[send_one(chat_id, post, keyword) for chat_id, post, keyword in tasks],
            return_exceptions=True
        )

        success_count = sum(1 for r in results if r is True)
        return success_count

    async def fetch_and_notify(self) -> None:
        """Fetch posts and send notifications"""
        try:
            # Always use the configured source (no fallback to RSS)
            logger.info(f"[{self.forum_id}] ğŸ“¡ å¼€å§‹æ‹‰å–æ•°æ® ({self.source.get_source_name()})...")
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„ fetchï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            loop = asyncio.get_event_loop()
            posts = await loop.run_in_executor(None, self.source.fetch)

            # Use cached data
            keywords = self._get_keywords_cached()
            subscribe_all_users = self._get_subscribe_all_users_cached()
            subscribe_all_set: Set[int] = set(subscribe_all_users)
            subscribed_authors = self._get_subscribed_authors_cached()

            new_posts = []
            pending_tasks: List[Tuple] = []  # (chat_id, post, keyword_or_none)

            for post in posts:
                # Skip if post already processed
                if self.db.post_exists(post.id, forum=self.forum_id):
                    continue

                new_posts.append(post)
                self.db.add_post(post, forum=self.forum_id)

                # Track users already notified for this post (in this cycle)
                notified_users: Set[int] = set()

                # Collect subscribe_all notifications
                for chat_id in subscribe_all_users:
                    # Check DB for existing notification
                    if self.db.notification_exists_for_post(chat_id, post.id, forum=self.forum_id):
                        notified_users.add(chat_id)
                        continue
                    pending_tasks.append((chat_id, post, None))
                    notified_users.add(chat_id)

                # Collect author-based notifications
                if post.author and subscribed_authors:
                    author_lower = post.author.lower()
                    if author_lower in [a.lower() for a in subscribed_authors]:
                        subscribers = self._get_author_subscribers_cached(author_lower)
                        for chat_id in subscribers:
                            # Skip if already notified
                            if chat_id in notified_users:
                                continue
                            if chat_id in subscribe_all_set:
                                continue
                            if self.db.notification_exists_for_post(chat_id, post.id, forum=self.forum_id):
                                notified_users.add(chat_id)
                                continue
                            # Use special keyword format for author subscription
                            pending_tasks.append((chat_id, post, f"@{post.author}"))
                            notified_users.add(chat_id)

                # Collect keyword-based notifications
                if keywords:
                    matched_keywords = self.matcher.find_matching_keywords(post, keywords)

                    for keyword in matched_keywords:
                        subscribers = self._get_subscribers_cached(keyword)

                        for chat_id in subscribers:
                            # Skip if already notified (subscribe_all or another keyword)
                            if chat_id in notified_users:
                                continue
                            # Skip if already in subscribe_all
                            if chat_id in subscribe_all_set:
                                continue
                            # Check DB for existing notification for this post
                            if self.db.notification_exists_for_post(chat_id, post.id, forum=self.forum_id):
                                notified_users.add(chat_id)
                                continue

                            pending_tasks.append((chat_id, post, keyword))
                            notified_users.add(chat_id)

            # Send notifications in batches
            total_sent = 0
            for i in range(0, len(pending_tasks), BATCH_SIZE):
                batch = pending_tasks[i:i + BATCH_SIZE]
                sent = await self._send_batch(batch)
                total_sent += sent

                if sent > 0:
                    logger.info(f"[{self.forum_id}]   ğŸ“¤ æ‰¹é‡å‘é€ {sent}/{len(batch)} æ¡")

                # Rate limit between batches
                if i + BATCH_SIZE < len(pending_tasks):
                    await asyncio.sleep(BATCH_INTERVAL)

            # Summary log
            logger.info(f"[{self.forum_id}] âœ… æ‹‰å–å®Œæˆ: å…± {len(posts)} æ¡, æ–°å¢ {len(new_posts)} æ¡, æ¨é€ {total_sent} æ¡é€šçŸ¥")

            # æ‹‰å–æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
            if self._fetch_fail_count > 0:
                logger.info(f"[{self.forum_id}] âœ… æ•°æ®æ‹‰å–æ¢å¤æ­£å¸¸ï¼ˆä¹‹å‰è¿ç»­å¤±è´¥ {self._fetch_fail_count} æ¬¡ï¼‰")
                if self._fetch_fail_notified:
                    await self._notify_admin(f"âœ… [{self.forum_name}] æ•°æ®æ‹‰å–å·²æ¢å¤æ­£å¸¸ï¼Œä¹‹å‰çš„å‘Šè­¦å¯ä»¥å¿½ç•¥äº†")
                self._fetch_fail_count = 0
                self._fetch_fail_notified = False

        except Exception as e:
            self._fetch_fail_count += 1
            logger.error(f"[{self.forum_id}] âŒ æ•°æ®æ‹‰å–å¤±è´¥ (ç¬¬ {self._fetch_fail_count} æ¬¡): {e}")

            # è¿ç»­å¤±è´¥è¾¾åˆ°é˜ˆå€¼æ—¶å‘é€å‘Šè­¦
            if self._fetch_fail_count >= self._fetch_fail_threshold and not self._fetch_fail_notified:
                self._fetch_fail_notified = True
                await self._notify_admin(
                    f"âš ï¸ [{self.forum_name}] æ•°æ®æ‹‰å–è¿ç»­å¤±è´¥ {self._fetch_fail_count} æ¬¡\n\n"
                    f"é”™è¯¯ä¿¡æ¯: {e}\n\n"
                    f"è¯·æ£€æŸ¥:\n"
                    f"1. FlareSolverr æœåŠ¡æ˜¯å¦æ­£å¸¸\n"
                    f"2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                    f"3. ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®"
                )

    def run(self) -> None:
        """Start the application (blocking, for single forum mode)"""
        # Setup bot
        application = self.bot.setup()

        # Job IDs are unique per forum
        data_fetch_job_id = f"data_fetch_{self.forum_id}"
        cookie_check_job_id = f"cookie_check_{self.forum_id}"

        # Schedule fetching
        # misfire_grace_time: å…è®¸å»¶è¿Ÿæ‰§è¡Œçš„æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNone è¡¨ç¤ºæ— é™
        # coalesce: å¦‚æœé”™è¿‡å¤šæ¬¡ï¼Œåªæ‰§è¡Œä¸€æ¬¡
        self.scheduler.add_job(
            self.fetch_and_notify,
            "interval",
            seconds=self.forum_config.fetch_interval,
            id=data_fetch_job_id,
            misfire_grace_time=None,
            coalesce=True
        )

        # Schedule cookie check (ç‹¬ç«‹ä»»åŠ¡)
        if self.forum_config.source_type == SourceType.DISCOURSE and self.forum_config.cookie_check_interval > 0:
            self.scheduler.add_job(
                self._check_cookie_task,
                "interval",
                seconds=self.forum_config.cookie_check_interval,
                id=cookie_check_job_id,
                misfire_grace_time=None,
                coalesce=True
            )

        # Run initial fetch after bot starts
        async def post_init(app):
            self.scheduler.start()
            logger.info(f"[{self.forum_id}] â° å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨, æ¯ {self.forum_config.fetch_interval} ç§’æ‹‰å–ä¸€æ¬¡")
            if self.forum_config.source_type == SourceType.DISCOURSE and self.forum_config.cookie_check_interval > 0:
                logger.info(f"[{self.forum_id}] ğŸ” Cookie æ£€æµ‹å·²å¯åŠ¨, æ¯ {self.forum_config.cookie_check_interval} ç§’æ£€æµ‹ä¸€æ¬¡")
            # Run initial fetch
            await self.fetch_and_notify()

        application.post_init = post_init

        # Start bot (blocking)
        logger.info(f"[{self.forum_id}] ğŸ¤– Telegram Bot å¯åŠ¨ä¸­...")
        application.run_polling()

    async def start_async(self) -> None:
        """Start the application asynchronously (for multi-forum mode)"""
        # Setup bot
        self.application = self.bot.setup()

        # Job IDs are unique per forum
        data_fetch_job_id = f"data_fetch_{self.forum_id}"
        cookie_check_job_id = f"cookie_check_{self.forum_id}"

        # Schedule fetching
        self.scheduler.add_job(
            self.fetch_and_notify,
            "interval",
            seconds=self.forum_config.fetch_interval,
            id=data_fetch_job_id,
            misfire_grace_time=None,
            coalesce=True
        )

        # Schedule cookie check
        if self.forum_config.source_type == SourceType.DISCOURSE and self.forum_config.cookie_check_interval > 0:
            self.scheduler.add_job(
                self._check_cookie_task,
                "interval",
                seconds=self.forum_config.cookie_check_interval,
                id=cookie_check_job_id,
                misfire_grace_time=None,
                coalesce=True
            )

        # Initialize and start bot
        await self.application.initialize()
        await self.application.start()
        await self.application.updater.start_polling()

        # Start scheduler
        self.scheduler.start()
        logger.info(f"[{self.forum_id}] ğŸ¤– Telegram Bot å·²å¯åŠ¨")
        logger.info(f"[{self.forum_id}] â° å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨, æ¯ {self.forum_config.fetch_interval} ç§’æ‹‰å–ä¸€æ¬¡")

        if self.forum_config.source_type == SourceType.DISCOURSE and self.forum_config.cookie_check_interval > 0:
            logger.info(f"[{self.forum_id}] ğŸ” Cookie æ£€æµ‹å·²å¯åŠ¨, æ¯ {self.forum_config.cookie_check_interval} ç§’æ£€æµ‹ä¸€æ¬¡")

        # Run initial fetch
        await self.fetch_and_notify()

    async def stop_async(self) -> None:
        """Stop the application asynchronously"""
        if hasattr(self, 'application') and self.application:
            try:
                await self.application.updater.stop()
                await self.application.stop()
                await self.application.shutdown()
            except Exception as e:
                logger.error(f"[{self.forum_id}] åœæ­¢ Bot æ—¶å‡ºé”™: {e}")
        if self.scheduler.running:
            self.scheduler.shutdown()
        logger.info(f"[{self.forum_id}] ğŸ›‘ å·²åœæ­¢")

    def is_running(self) -> bool:
        """Check if the application is running"""
        if not hasattr(self, 'application') or not self.application:
            return False
        if not self.application.updater:
            return False
        return self.application.updater.running

    def _reset_state(self) -> None:
        """Reset application state for restart"""
        # Reset failure counters
        self._cookie_fail_count = 0
        self._cookie_notify_round = 0
        self._fetch_fail_count = 0
        self._fetch_fail_notified = False

        # Clear cache
        self.cache.clear_all()

        # Recreate cache with forum isolation
        self.cache = AppCache(forum_id=self.forum_id)

        # Recreate scheduler (old one is shutdown)
        self.scheduler = AsyncIOScheduler()

        # Recreate bot and source
        self.bot = TelegramBot(
            self.forum_config.bot_token,
            self.db,
            forum_id=self.forum_id,
            forum_name=self.forum_name,
            cache=self.cache  # Pass shared cache
        )
        self.source = create_source(self.forum_config)

        # Clear application reference
        self.application = None


class MultiForumApplication:
    """Manages multiple forum applications running in parallel with fault isolation"""

    def __init__(
        self,
        config: AppConfig,
        db: Database,
        config_manager: Optional[ConfigManager] = None
    ):
        self.config = config
        self.db = db
        self.config_manager = config_manager
        self.apps: List[Application] = []
        self._running = False
        self._tasks: List[asyncio.Task] = []

    def _create_apps(self) -> None:
        """Create Application instances for each enabled forum"""
        self.apps = []
        for forum_config in self.config.get_enabled_forums():
            app = Application(
                forum_config=forum_config,
                db=self.db,
                admin_chat_id=self.config.admin_chat_id,
                config_manager=self.config_manager
            )
            self.apps.append(app)

    def reload_config(self) -> None:
        """Hot reload configuration for all apps"""
        for app in self.apps:
            try:
                app.reload_config()
            except Exception as e:
                logger.error(f"[{app.forum_id}] çƒ­æ›´æ–°å¤±è´¥: {e}")

    def run(self) -> None:
        """Start all forum applications"""
        self._create_apps()

        if not self.apps:
            logger.error("æ²¡æœ‰å¯ç”¨çš„è®ºå›é…ç½®")
            return

        if len(self.apps) == 1:
            # Single forum - use blocking mode
            logger.info(f"ğŸš€ å¯åŠ¨å•è®ºå›æ¨¡å¼: {self.apps[0].forum_name}")
            self.apps[0].run()
        else:
            # Multiple forums - use async mode
            logger.info(f"ğŸš€ å¯åŠ¨å¤šè®ºå›æ¨¡å¼: {len(self.apps)} ä¸ªè®ºå›")
            asyncio.run(self._run_multi_async())

    async def _run_single_app(self, app: Application) -> None:
        """Run a single app with automatic restart on failure"""
        restart_delay = 5  # seconds
        max_restart_delay = 300  # 5 minutes max

        while self._running:
            try:
                logger.info(f"[{app.forum_id}] ğŸš€ å¯åŠ¨ä¸­...")
                await app.start_async()

                # Keep running until stopped or error
                while self._running and app.is_running():
                    await asyncio.sleep(1)

                if not self._running:
                    break

                logger.warning(f"[{app.forum_id}] âš ï¸ Bot æ„å¤–åœæ­¢ï¼Œå°†åœ¨ {restart_delay} ç§’åé‡å¯")

            except Exception as e:
                logger.error(f"[{app.forum_id}] âŒ è¿è¡Œå‡ºé”™: {e}")
                logger.warning(f"[{app.forum_id}] å°†åœ¨ {restart_delay} ç§’åé‡å¯")

            # Stop and cleanup
            try:
                await app.stop_async()
            except Exception as e:
                logger.error(f"[{app.forum_id}] åœæ­¢æ—¶å‡ºé”™: {e}")

            if not self._running:
                break

            # Wait before restart
            await asyncio.sleep(restart_delay)

            # Exponential backoff (cap at max_restart_delay)
            restart_delay = min(restart_delay * 2, max_restart_delay)

            # Reset app state for restart
            app._reset_state()

        logger.info(f"[{app.forum_id}] ğŸ›‘ å·²åœæ­¢")

    async def _run_multi_async(self) -> None:
        """Run multiple forums asynchronously with fault isolation"""
        self._running = True

        # Start each app in its own task (isolated)
        self._tasks = []
        for app in self.apps:
            task = asyncio.create_task(
                self._run_single_app(app),
                name=f"forum_{app.forum_id}"
            )
            self._tasks.append(task)

        logger.info(f"âœ… å·²å¯åŠ¨ {len(self._tasks)} ä¸ªè®ºå›ä»»åŠ¡")

        # Wait for shutdown signal
        try:
            while self._running:
                await asyncio.sleep(1)

                # Check if all tasks died
                alive_tasks = [t for t in self._tasks if not t.done()]
                if not alive_tasks:
                    logger.error("âŒ æ‰€æœ‰è®ºå›ä»»åŠ¡éƒ½å·²åœæ­¢")
                    break

        except asyncio.CancelledError:
            logger.info("æ”¶åˆ°å–æ¶ˆä¿¡å·")
        finally:
            self._running = False

            # Cancel all tasks
            for task in self._tasks:
                if not task.done():
                    task.cancel()

            # Wait for all tasks to complete
            if self._tasks:
                await asyncio.gather(*self._tasks, return_exceptions=True)

            logger.info("ğŸ›‘ æ‰€æœ‰è®ºå›å·²åœæ­¢")
